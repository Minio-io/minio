package cmd

import (
	"crypto/sha256"
	"errors"
	"fmt"
	"io"
	"io/ioutil"
	"os"
	"path"
	"strings"
	"syscall"
	"time"

	"github.com/minio/minio/pkg/disk"

	"encoding/json"
)

// Metadata of a cached object.
type diskCacheObjectMeta struct {
	// Version of the metadata structure
	Version string `json:"version"`
	// Name of the bucket of the cached object
	BucketName string `json:"bucketName"`
	// Name of the cached object
	ObjectName string `json:"objectName"`
	// Used for "Last-Modified" header
	ModTime time.Time `json:"modTime"`
	// Used for Content-Length
	Size int64 `json:"size"`
	// True if the object was cached on an anonymous request
	// Used when the backend is down and we have to know if
	// we can serve the current anonymous request
	Anonymous bool `json:"anonymous"`
	// Other user defined metadata
	HTTPMeta map[string]string `json:"httpMeta"`
}

// Converts diskCacheObjectMeta to ObjectInfo.
func (objectMeta diskCacheObjectMeta) toObjectInfo() ObjectInfo {
	objInfo := ObjectInfo{}
	objInfo.UserDefined = make(map[string]string)
	objInfo.Bucket = objectMeta.BucketName
	objInfo.Name = objectMeta.ObjectName

	objInfo.ModTime = objectMeta.ModTime
	objInfo.Size = objectMeta.Size
	objInfo.MD5Sum = objectMeta.HTTPMeta["md5Sum"]
	objInfo.ContentType = objectMeta.HTTPMeta["content-type"]
	objInfo.ContentEncoding = objectMeta.HTTPMeta["content-encoding"]

	for key, val := range objectMeta.HTTPMeta {
		if key == "md5Sum" {
			continue
		}
		objInfo.UserDefined[key] = val
	}

	return objInfo
}

// Converts objInfo to diskCacheObjectMeta.
func newDiskCacheObjectMeta(objInfo ObjectInfo, anon bool) diskCacheObjectMeta {
	objMeta := diskCacheObjectMeta{}
	objMeta.HTTPMeta = make(map[string]string)
	objMeta.BucketName = objInfo.Bucket
	objMeta.ObjectName = objInfo.Name
	objMeta.ModTime = objInfo.ModTime
	objMeta.Size = objInfo.Size

	objMeta.HTTPMeta["md5Sum"] = objInfo.MD5Sum
	objMeta.HTTPMeta["content-type"] = objInfo.ContentType
	objMeta.HTTPMeta["content-encoding"] = objInfo.ContentEncoding

	for key, val := range objInfo.UserDefined {
		objMeta.HTTPMeta[key] = val
	}

	objMeta.Anonymous = anon

	return objMeta
}

// Disk caching
type diskCache struct {
	dir string // caching directory (--cache-dir)
	// Max disk usage limit (--cache-max) in percent. If the disk usage crosses this
	// limit then we stop caching.
	maxUsage int
	// Cache revalidation (in days)
	expiry int
	// for storing temporary files
	tmpDir string
	// for storing cached objects (renamed from tmp location)
	dataDir string
	// creation-time of the cache
	createTime time.Time

	// purge() listens on this channel to start the cache-purge process
	purgeChan chan struct{}
}

// Channel entry generated by diskCache.getReadDirCh()
type diskCacheReadDirInfo struct {
	entry string
	err   error
}

// For listing the files in the data directory. Used for purging unused cached objects.
func (c diskCache) getReadDirCh() chan diskCacheReadDirInfo {
	ch := make(chan diskCacheReadDirInfo)
	go func() {
		bufp := readDirBufPool.Get().(*[]byte)
		buf := *bufp
		defer readDirBufPool.Put(bufp)
		defer close(ch)

		d, err := os.Open(c.dataDir)
		if err != nil {
			if os.IsNotExist(err) {
				ch <- diskCacheReadDirInfo{"", errFileNotFound}
				return
			}
			if os.IsPermission(err) {
				ch <- diskCacheReadDirInfo{"", errFileAccessDenied}
				return
			}

			ch <- diskCacheReadDirInfo{"", err}
			return
		}
		defer d.Close()

		fd := int(d.Fd())
		for {
			nbuf, err := syscall.ReadDirent(fd, buf)
			if err != nil {
				ch <- diskCacheReadDirInfo{"", err}
				return
			}
			if nbuf <= 0 {
				break
			}
			var tmpEntries []string
			if tmpEntries, err = parseDirents(c.dataDir, buf[:nbuf]); err != nil {
				ch <- diskCacheReadDirInfo{"", err}
				return
			}
			for _, entry := range tmpEntries {
				ch <- diskCacheReadDirInfo{entry, nil}
			}
		}
	}()
	return ch
}

// Returns if the disk usage is low.
// Disk usage is low if usage is < 80% of maxUsage
// Ex. for a 100GB disk, if maxUsage is configured as 70% then maxUsage is 70G
// hence disk usage is low if the disk usage is less than 56G (because 80% of 70G is 56G)
func (c diskCache) diskUsageLow() bool {
	minUsage := c.maxUsage * 80 / 100

	di, err := disk.GetInfo(c.dir)
	if err != nil {
		errorIf(err, "Error getting disk information on %s", c.dir)
		return false
	}
	usedPercent := (di.Total - di.Free) * 100 / di.Total
	return int(usedPercent) < minUsage
}

// Retusn if the disk usage is high.
// Disk usage is high if disk used is > maxUsage
func (c diskCache) diskUsageHigh() bool {
	di, err := disk.GetInfo(c.dir)
	if err != nil {
		return true
	}
	usedPercent := (di.Total - di.Free) * 100 / di.Total
	return int(usedPercent) > c.maxUsage
}

// Run as a go-routine. Used for purging unused cached objects.
func (c diskCache) purge() {
	expiry := c.createTime
	for {
		for {
			if c.diskUsageLow() {
				break
			}
			d := time.Now().UTC().Sub(expiry)
			d = d / 2
			if d < time.Second {
				break
			}
			expiry = expiry.Add(d)
			ch := c.getReadDirCh()
			for info := range ch {
				if info.err != nil {
					break
				}

				entryPath := pathJoin(c.dataDir, info.entry)
				if strings.HasSuffix(entryPath, ".json") {
					continue
				}
				fi, err := os.Stat(entryPath)
				if err != nil {
					errorIf(err, "Unable to Stat %s", entryPath)
					continue
				}
				stat := fi.Sys().(*syscall.Stat_t)
				atime := time.Unix(int64(stat.Atim.Sec), int64(stat.Atim.Nsec)).UTC()
				if atime.After(expiry) {
					continue
				}
				if err = os.Remove(entryPath); err != nil {
					errorIf(err, "Error removing %s", entryPath)
					continue
				}
				if err = os.Remove(entryPath + ".json"); err != nil {
					errorIf(err, "Error removing %s", entryPath+".json")
					continue
				}
			}
		}
		<-c.purgeChan
	}
}

// encode (bucket,object) to a hash value which is used as the file name of the cached object.
func (c diskCache) encodedPath(bucket, object string) string {
	return path.Join(c.dataDir, fmt.Sprintf("%x", sha256.Sum256([]byte(path.Join(bucket, object)))))
}

// encode (bucket,object) to a hash value which is used as the file name of the cached object's metadata.
func (c diskCache) encodedMetaPath(bucket, object string) string {
	return path.Join(c.dataDir, fmt.Sprintf("%x.json", sha256.Sum256([]byte(path.Join(bucket, object)))))
}

// Commit the cached object - rename from tmp directory to data directory.
func (c diskCache) Commit(f *os.File, objInfo ObjectInfo, anon bool) error {
	encPath := c.encodedPath(objInfo.Bucket, objInfo.Name)
	if err := os.Rename(f.Name(), encPath); err != nil {
		return err
	}
	if err := f.Close(); err != nil {
		return err
	}
	metaPath := c.encodedMetaPath(objInfo.Bucket, objInfo.Name)
	objMeta := newDiskCacheObjectMeta(objInfo, anon)
	metaBytes, err := json.Marshal(objMeta)
	if err != nil {
		return err
	}
	// FIXME: take care of locking.
	if err = ioutil.WriteFile(metaPath, metaBytes, 0644); err != nil {
		return err
	}
	return nil
}

// Remove the cached object from the tmp directory.
func (o diskCache) NoCommit(f *os.File) error {
	tmpName := f.Name()
	if err := f.Close(); err != nil {
		return err
	}

	return os.Remove(tmpName)
}

// Creates a file in the tmp directory to which the cached object is written to.
// Once the object is written to disk the caller can call diskCache.Commit()
func (c diskCache) Put() (*os.File, error) {
	if c.diskUsageHigh() {
		select {
		case c.purgeChan <- struct{}{}:
		default:
		}
		return nil, errDiskFull
	}
	return ioutil.TempFile(c.tmpDir, "")
}

// Returns the handle for the cached object
func (c diskCache) Get(bucket, object string) (*os.File, ObjectInfo, bool, error) {
	metaPath := c.encodedMetaPath(bucket, object)
	objMeta := diskCacheObjectMeta{}
	metaBytes, err := ioutil.ReadFile(metaPath)
	if err != nil {
		return nil, ObjectInfo{}, false, err
	}
	if err := json.Unmarshal(metaBytes, &objMeta); err != nil {
		return nil, ObjectInfo{}, false, err
	}

	file, err := os.Open(c.encodedPath(bucket, object))
	if err != nil {
		return nil, ObjectInfo{}, false, err
	}
	return file, objMeta.toObjectInfo(), objMeta.Anonymous, nil
}

// Returns metadata of the cached object
func (c diskCache) GetObjectInfo(bucket, object string) (ObjectInfo, bool, error) {
	metaPath := c.encodedMetaPath(bucket, object)
	objMeta := diskCacheObjectMeta{}
	metaBytes, err := ioutil.ReadFile(metaPath)
	if err != nil {
		return ObjectInfo{}, false, err
	}
	if err := json.Unmarshal(metaBytes, &objMeta); err != nil {
		return ObjectInfo{}, false, err
	}

	return objMeta.toObjectInfo(), objMeta.Anonymous, nil
}

// Deletes the cached object
func (c diskCache) Delete(bucket, object string) error {
	if err := os.Remove(c.encodedPath(bucket, object)); err != nil {
		return err
	}
	if err := os.Remove(c.encodedMetaPath(bucket, object)); err != nil {
		return err
	}
	return nil
}

// format.json structure
// format.json is put in the root of the cache directory
type diskCacheFormat struct {
	Version int       `json:"version"`
	Format  string    `json:"format"`
	Time    time.Time `json:"createTime"`
}

// Inits the cache-dir if it is not init'ed already.
// Initializing implies creation of cache-dir, data-dir, tmp-dir and format.json
func newDiskCache(dir string, maxUsage, expiry int) (*diskCache, error) {
	if err := os.MkdirAll(dir, 0766); err != nil {
		return nil, err
	}
	formatPath := path.Join(dir, "format.json")
	formatBytes, err := ioutil.ReadFile(formatPath)
	var format diskCacheFormat
	if err != nil {
		if !os.IsNotExist(err) {
			return nil, err
		}
		format.Version = 1
		format.Format = "cachefs"
		format.Time = time.Now().UTC()
		if formatBytes, err = json.Marshal(format); err != nil {
			return nil, err
		}
		if err = ioutil.WriteFile(formatPath, formatBytes, 0644); err != nil {
			return nil, err
		}
	} else {
		if err = json.Unmarshal(formatBytes, &format); err != nil {
			return nil, err
		}
		if format.Version != 1 {
			return nil, errors.New("format not supported")
		}
	}

	tmpDir := path.Join(dir, "tmp")
	dataDir := path.Join(dir, "data")
	if err := os.MkdirAll(tmpDir, 0766); err != nil {
		return nil, err
	}
	if err := os.MkdirAll(dataDir, 0766); err != nil {
		return nil, err
	}
	cache := &diskCache{dir, maxUsage, expiry, tmpDir, dataDir, format.Time, make(chan struct{})}

	// Start the purging go-routine
	go cache.purge()
	return cache, nil
}

// Abstracts disk caching - used by the S3 layer
type cacheObjects struct {
	dcache *diskCache

	GetObjectFn         func(bucket, object string, startOffset int64, length int64, writer io.Writer) (err error)
	GetObjectInfoFn     func(bucket, object string) (objInfo ObjectInfo, err error)
	AnonGetObjectFn     func(bucket, object string, startOffset int64, length int64, writer io.Writer) (err error)
	AnonGetObjectInfoFn func(bucket, object string) (objInfo ObjectInfo, err error)
}

func (c cacheObjects) getObject(bucket, object string, startOffset int64, length int64, writer io.Writer, anonReq bool) (err error) {
	GetObjectFn := c.GetObjectFn
	GetObjectInfoFn := c.GetObjectInfoFn
	GetObjectInfo := c.GetObjectInfo
	if anonReq {
		GetObjectFn = c.AnonGetObjectFn
		GetObjectInfoFn = c.AnonGetObjectInfoFn
		GetObjectInfo = c.AnonGetObjectInfo
	}
	objInfo, err := GetObjectInfo(bucket, object)
	_, backendDown := errorCause(err).(BackendDown)
	if err != nil && !backendDown {
		// Do not delete cache entry
		return err
	}
	r, cachedObjInfo, anon, err := c.dcache.Get(bucket, object)
	if err == nil {
		defer r.Close()
		if backendDown {
			if anonReq {
				if anon {
					_, err = io.Copy(writer, io.NewSectionReader(r, startOffset, length))
					return err
				} else {
					return BackendDown{}
				}
			} else {
				// If the backend is down, serve the request from cache.
				_, err = io.Copy(writer, io.NewSectionReader(r, startOffset, length))
				return err
			}
		} else {
			if cachedObjInfo.MD5Sum == objInfo.MD5Sum {
				_, err = io.Copy(writer, io.NewSectionReader(r, startOffset, length))
				return err
			} else {
				c.dcache.Delete(bucket, object)
			}
		}
	}
	if startOffset != 0 || length != objInfo.Size {
		return GetObjectFn(bucket, object, startOffset, length, writer)
	}
	cachedObj, err := c.dcache.Put()
	if err == errDiskFull {
		return GetObjectFn(bucket, object, 0, objInfo.Size, writer)
	}
	if err != nil {
		return err
	}
	err = GetObjectFn(bucket, object, 0, objInfo.Size, io.MultiWriter(writer, cachedObj))
	if err != nil {
		c.dcache.NoCommit(cachedObj)
		return err
	}
	objInfo, err = GetObjectInfoFn(bucket, object)
	if err != nil {
		c.dcache.NoCommit(cachedObj)
		return err
	}
	// FIXME: race-condition: what if the server object got replaced.
	return c.dcache.Commit(cachedObj, objInfo, true)
}

// Uses cached-object to serve the request. If object is not cached it serves the request from the backend and also
// stores it in the cache for serving subsequent requests.
func (c cacheObjects) GetObject(bucket, object string, startOffset int64, length int64, writer io.Writer) (err error) {
	return c.getObject(bucket, object, startOffset, length, writer, false)
}

// Anonymous version of cacheObjects.GetObject
func (c cacheObjects) AnonGetObject(bucket, object string, startOffset int64, length int64, writer io.Writer) (err error) {
	return c.getObject(bucket, object, startOffset, length, writer, true)
}

// Returns ObjectInfo from cache or the backend.
func (c cacheObjects) GetObjectInfo(bucket, object string) (ObjectInfo, error) {
	objInfo, err := c.GetObjectInfoFn(bucket, object)
	if err != nil {
		if _, ok := errorCause(err).(BackendDown); !ok {
			c.dcache.Delete(bucket, object)
			return ObjectInfo{}, err
		}
		cachedObjInfo, _, err := c.dcache.GetObjectInfo(bucket, object)
		if err == nil {
			return cachedObjInfo, nil
		}
		return ObjectInfo{}, BackendDown{}
	}
	cachedObjInfo, _, err := c.dcache.GetObjectInfo(bucket, object)
	if err != nil {
		return objInfo, nil
	}

	if cachedObjInfo.MD5Sum != objInfo.MD5Sum {
		c.dcache.Delete(bucket, object)
	}
	return objInfo, nil
}

// Anonymous version of cacheObjects.GetObjectInfo
func (c cacheObjects) AnonGetObjectInfo(bucket, object string) (ObjectInfo, error) {
	objInfo, err := c.AnonGetObjectInfoFn(bucket, object)
	if err != nil {
		if _, ok := errorCause(err).(BackendDown); !ok {
			return ObjectInfo{}, err
		}
		cachedObjInfo, anon, err := c.dcache.GetObjectInfo(bucket, object)
		if err == nil && anon {
			return cachedObjInfo, nil
		}
		return ObjectInfo{}, BackendDown{}
	}
	cachedObjInfo, _, err := c.dcache.GetObjectInfo(bucket, object)
	if err != nil {
		return objInfo, nil
	}
	if cachedObjInfo.MD5Sum != objInfo.MD5Sum {
		c.dcache.Delete(bucket, object)
	}
	return objInfo, nil
}

// Returns cachedObjects for use by Server.
func newServerCacheObjects(l ObjectLayer, dir string, maxUsage, expiry int) (*cacheObjects, error) {
	dcache, err := newDiskCache(dir, maxUsage, expiry)
	if err != nil {
		return nil, err
	}

	return &cacheObjects{
		dcache:          dcache,
		GetObjectFn:     l.GetObject,
		GetObjectInfoFn: l.GetObjectInfo,
		AnonGetObjectFn: func(bucket, object string, startOffset int64, length int64, writer io.Writer) error {
			return NotImplemented{}
		},
		AnonGetObjectInfoFn: func(bucket, object string) (ObjectInfo, error) {
			return ObjectInfo{}, NotImplemented{}
		},
	}, nil
}

// Returns cachedObjects for use by Gateway.
func newGatewayCacheObjects(l GatewayLayer, dir string, maxUsage, expiry int) (*cacheObjects, error) {
	dcache, err := newDiskCache(dir, maxUsage, expiry)
	if err != nil {
		return nil, err
	}

	return &cacheObjects{
		dcache:              dcache,
		GetObjectFn:         l.GetObject,
		GetObjectInfoFn:     l.GetObjectInfo,
		AnonGetObjectFn:     l.AnonGetObject,
		AnonGetObjectInfoFn: l.AnonGetObjectInfo,
	}, nil
}
